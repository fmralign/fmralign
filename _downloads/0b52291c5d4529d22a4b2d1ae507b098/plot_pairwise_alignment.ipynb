{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pairwise functional alignment.\nThis is a comment\nIn this tutorial, we show how to better predict new contrasts for a target\nsubject using source subject corresponding contrasts and data in common.\n\nWe mostly rely on python common packages and on nilearn to handle functional\ndata in a clean fashion.\n\n\nTo run this example, you must launch IPython via ``ipython\n--matplotlib`` in a terminal, or use ``jupyter-notebook``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the data\nIn this example we use the IBC dataset, which include a large number of\ndifferent contrasts maps for 12 subjects. We download the images for\nsubjects sub-01 and sub-02 (or retrieve them if they were already downloaded)\nFiles is the list of paths for each subjects.\ndf is a dataframe with metadata about each of them.\nmask is an appropriate nifti image to select the data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.fetch_example_data import fetch_ibc_subjects_contrasts\n\nfiles, df, mask = fetch_ibc_subjects_contrasts([\"sub-01\", \"sub-02\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a masker\nWe define a nilearn masker that will be used to handle relevant data.\n  For more information, visit :\n  'http://nilearn.github.io/manipulating_images/masker_objects.html'\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.image import concat_imgs\nfrom nilearn.maskers import MultiNiftiMasker\n\nmasker = MultiNiftiMasker(mask_img=mask).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\nFor each subject, for each task and conditions, our dataset contains two\nindependent acquisitions, similar except for one acquisition parameter, the\nencoding phase used that was either Antero-Posterior (AP) or Postero-Anterior (PA).\n\nAlthough this induces small differences in the final data, we will take\nadvantage of these \"duplicates to create a training and a testing set that\ncontains roughly the same signals but acquired totally independently.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The training fold, used to learn alignment from source subject toward target:\n# * source train: AP contrasts for subject sub-01\n# * target train: AP contrasts for subject sub-02\n\nsource_train_imgs = concat_imgs(\n    df[(df.subject == \"sub-01\") & (df.acquisition == \"ap\")].path.values\n)\ntarget_train_imgs = concat_imgs(\n    df[(df.subject == \"sub-02\") & (df.acquisition == \"ap\")].path.values\n)\n\n# The testing fold:\n# * source test: PA contrasts for subject sub-01, used to predict\n#   the corresponding contrasts of subject sub-02\n# * target test: PA contrasts for subject sub-02, used as a ground truth\n#   to score our predictions\n\nsource_test_imgs = concat_imgs(\n    df[(df.subject == \"sub-01\") & (df.acquisition == \"pa\")].path.values\n)\ntarget_test_imgs = concat_imgs(\n    df[(df.subject == \"sub-02\") & (df.acquisition == \"pa\")].path.values\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate a parcellation from the images\nWe will compute the alignment in a piecewise manner, that is, we will align\nthe data in small parcels of the brain, which are groups of functionally\nsimilar voxels. To do so, we need to generate a parcellation of the\nfunctional data. We use the :func:`!fmralign.embeddings.parcellation.get_labels`\nutility, which will generate a parcellation of the data in 150 pieces.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.embeddings.parcellation import get_labels\n\nlabels = get_labels(\n    imgs=[source_train_imgs, target_train_imgs],\n    n_pieces=150,\n    masker=masker,\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the estimator, fit it and predict\nTo proceed with the alignment we use :class:`fmralign.alignment.pairwise_alignment.PairwiseAlignment`,\nwhich implements various functional alignment methods between data from two\nsubjects. In this example, we use the Procrustes method. Since we want to\nalign the data in parcels, we pass the labels we just computed to the\nestimator. The labels are used to compute the alignment in each parcel\nseparately, and then to aggregate the local transformations into a global\ntransformation that is applied to the whole brain.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign import PairwiseAlignment\n\n(source_train_data, target_train_data, source_test_data, target_test_data) = (\n    masker.transform(\n        [\n            source_train_imgs,\n            target_train_imgs,\n            source_test_imgs,\n            target_test_imgs,\n        ]\n    )\n)\n\nalignment_estimator = PairwiseAlignment(method=\"procrustes\", labels=labels)\n# Learn alignment operator from subject 1 to subject 2 on training data\nalignment_estimator.fit(source_train_data, target_train_data)\n# Predict test data for subject 2 from subject 1\ntarget_pred_data = alignment_estimator.transform(source_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score the baseline and the prediction\nWe use a utility scoring function to measure the voxelwise correlation between\nthe prediction and the ground truth. That is, for each voxel, we measure the\ncorrelation between its profile of activation without and with alignment,\nto see if alignment was able to predict a signal more alike the ground truth.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.metrics import score_voxelwise\n\n# Now we use this scoring function to compare the correlation of aligned and\n# original data from sub-01 made with the real PA contrasts of sub-02.\n\ntarget_pred_imgs = masker.inverse_transform(target_pred_data)\nbaseline_score = score_voxelwise(\n    target_test_data, source_test_data, loss=\"corr\"\n)\naligned_score = score_voxelwise(\n    target_test_data, target_pred_data, loss=\"corr\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the measures\nFinally we plot both scores\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n\nbaseline_score_img = masker.inverse_transform(baseline_score)\naligned_score_img = masker.inverse_transform(aligned_score)\nbaseline_display = plotting.plot_stat_map(\n    baseline_score_img, display_mode=\"z\", vmax=1, cut_coords=[-15, -5]\n)\nbaseline_display.title(\"Baseline correlation wt ground truth\")\ndisplay = plotting.plot_stat_map(\n    aligned_score_img, display_mode=\"z\", cut_coords=[-15, -5], vmax=1\n)\ndisplay.title(\"Prediction correlation wt ground truth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see on the plot that after alignment the prediction made for one\nsubject data, informed by another subject are greatly improved.\n\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}