{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Pairwise functional alignment on a ROI.\n\nIn this tutorial, we focus here on a single Region of Interest (ROI). On this\nROI, we try to find a transformation from source to target subject that\ncaptures the variability between their signal on data they share. We then use\nthis transformation to predict new contrasts for the target subject.\n\nWe mostly rely on python common packages and on nilearn to handle functional\ndata in a clean fashion.\n\n\nTo run this example, you must launch IPython via ``ipython\n--matplotlib`` in a terminal, or use ``jupyter-notebook``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the data\nIn this example we use the IBC dataset, which include a large number of\ndifferent contrasts maps for 12 subjects.\nWe download the images for subjects sub-01 and sub-02 (or retrieve them if they\nwere already downloaded).\nFiles is the list of paths for each subjects.\ndf is a dataframe with metadata about each of them.\nmask is an appropriate nifti image to select the data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.fetch_example_data import fetch_ibc_subjects_contrasts\n\nfiles, df, mask = fetch_ibc_subjects_contrasts([\"sub-01\", \"sub-02\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract a mask for the visual cortex from Yeo Atlas\nFirst, we fetch and plot the complete atlas\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets\nfrom nilearn.image import concat_imgs, load_img, new_img_like, resample_to_img\nfrom nilearn.plotting import plot_roi\n\natlas_yeo_2011 = datasets.fetch_atlas_yeo_2011()\natlas_yeo = atlas_yeo_2011.thick_7\natlas = load_img(atlas_yeo)\n\n# Select visual cortex, create a mask and resample it to the right resolution\n\nmask_visual = new_img_like(atlas, atlas.get_fdata() == 1)\nresampled_mask_visual = resample_to_img(\n    mask_visual, mask, interpolation=\"nearest\"\n)\n\n# Plot the mask we will use\nplot_roi(\n    resampled_mask_visual,\n    title=\"Visual regions mask extracted from atlas\",\n    cut_coords=(8, -80, 9),\n    colorbar=True,\n    cmap=\"Paired\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a masker\nWe define a nilearn masker that will be used to handle relevant data.\nFor more information, consult Nilearn's documentation on\n:external+nilearn`masker objects <masker_objects>`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.maskers import MultiNiftiMasker\n\nroi_masker = MultiNiftiMasker(mask_img=resampled_mask_visual).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\nFor each subject, we will use two series of contrasts acquired during\ntwo independent sessions with a different phase encoding:\nAntero-posterior(AP) or Postero-anterior(PA).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The training fold, used to learn alignment from source subject toward target:\n# * source train: AP contrasts for subject sub-01\n# * target train: AP contrasts for subject sub-02\n\nsource_train_imgs = concat_imgs(\n    df[(df.subject == \"sub-01\") & (df.acquisition == \"ap\")].path.values\n)\ntarget_train_imgs = concat_imgs(\n    df[(df.subject == \"sub-02\") & (df.acquisition == \"ap\")].path.values\n)\n\n# The testing fold:\n# * source test: PA contrasts for subject sub-01, used to predict\n#   the corresponding contrasts of subject sub-02\n# * target test: PA contrasts for subject sub-02, used as a ground truth\n#   to score our predictions\n\nsource_test_imgs = concat_imgs(\n    df[(df.subject == \"sub-01\") & (df.acquisition == \"pa\")].path.values\n)\ntarget_test_imgs = concat_imgs(\n    df[(df.subject == \"sub-02\") & (df.acquisition == \"pa\")].path.values\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the estimator, fit it and predict\nTo proceed with alignment, we use\n:class:`~fmralign.alignment.pairwise_alignment.PairwiseAlignment`\nwith the visual mask we created before.\nWe use the :class:`~fmralign.methods.Procrustes` method,\nproposed in :footcite:t:`Haxby2011` under the name \"hyperalignment.\"\nAs we work on a single ROI, we will search correspondence\nbetween the full data of each subject and so we set the number of cluster\nn_pieces to 1. We learn alignment estimator on train data and use it to\npredict target test data.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign import PairwiseAlignment\n\n(\n    source_train_data,\n    target_train_data,\n    source_test_data,\n    target_test_data,\n) = roi_masker.transform(\n    [\n        source_train_imgs,\n        target_train_imgs,\n        source_test_imgs,\n        target_test_imgs,\n    ]\n)\n\nalignment_estimator = PairwiseAlignment(method=\"procrustes\")\nalignment_estimator.fit(source_train_data, target_train_data)\ntarget_pred_data = alignment_estimator.transform(source_test_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Score the baseline and the prediction\nWe use a utility scoring function to measure the voxelwise\ncorrelation between the prediction and the ground truth. That is, for each voxel,\nwe measure the correlation between its profile of activation without\nand with alignment, to see if alignment was able to predict a signal more\nalike the ground truth.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.metrics import score_voxelwise\n\n# Now we use this scoring function to compare the correlation of aligned and\n# original data from sub-01 made with the real PA contrasts of sub-02.\n\nbaseline_score = score_voxelwise(\n    target_test_data, source_test_data, loss=\"corr\"\n)\naligned_score = score_voxelwise(\n    target_test_data, target_pred_data, loss=\"corr\"\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the measures\nFinally we plot both scores.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import plotting\n\nbaseline_score_img = roi_masker.inverse_transform(baseline_score)\naligned_score_img = roi_masker.inverse_transform(aligned_score)\nbaseline_display = plotting.plot_stat_map(\n    baseline_score_img, display_mode=\"z\", vmax=1, cut_coords=[-15, -5]\n)\nbaseline_display.title(\"Baseline correlation wt ground truth\")\ndisplay = plotting.plot_stat_map(\n    aligned_score_img, display_mode=\"z\", cut_coords=[-15, -5], vmax=1\n)\ndisplay.title(\"Prediction correlation wt ground truth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see on the plot that after alignment, the prediction made for one\nsubject data, informed by another subject are greatly improved.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}