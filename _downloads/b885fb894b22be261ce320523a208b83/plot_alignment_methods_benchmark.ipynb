{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Alignment methods benchmark (template-based ROI case)\n\nIn this tutorial, we compare various methods of alignment on a template-based alignment\nproblem for Individual Brain Charting subjects. For each subject, we have a lot\nof functional information in the form of several task-based\ncontrast per subject. We will just work here on a ROI.\n\nWe mostly rely on python common packages and on nilearn to handle functional\ndata in a clean fashion.\n\nTo run this example, you must launch IPython via ``ipython --matplotlib`` in\na terminal, or use ``jupyter-notebook``.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Retrieve the data\nIn this example we use the IBC dataset, which include a large number of\ndifferent contrasts maps for 12 subjects.\n\nThe contrasts come from tasks in the Archi and HCP fMRI batteries, designed\nto probe a range of cognitive functions, including:\n\n* Motor control: finger, foot, and tongue movements\n* Social cognition: interpreting short films and stories\n* Spatial orientation: judging orientation and hand-side\n* Numerical reasoning: reading and listening to math problems\n* Emotion processing: judging facial expressions\n* Reward processing: responding to gains and losses\n* Working memory: maintaining sequences of faces and objects\n* Object categorization: matching and comparing visual stimuli\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# We download the images for subjects sub-01 and sub-02.\n# ``files`` is the list of paths for each subjects.\n# ``df`` is a dataframe with metadata about each of them.\n# ``mask`` is the common mask for IBC subjects.\n\nfrom fmralign.fetch_example_data import fetch_ibc_subjects_contrasts\n\nsub_ids = [\"sub-01\", \"sub-02\", \"sub-04\"]\nfiles, df, mask = fetch_ibc_subjects_contrasts(sub_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract a mask for the visual cortex from Yeo Atlas\nFirst, we fetch and plot the complete atlas\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn import datasets, plotting\nfrom nilearn.image import concat_imgs, load_img, new_img_like, resample_to_img\n\natlas_yeo_2011 = datasets.fetch_atlas_yeo_2011()\natlas = load_img(atlas_yeo_2011.thick_7)\n\n# Select visual cortex, create a mask and resample it to the right resolution\n\nmask_visual = new_img_like(atlas, atlas.get_fdata() == 1)\nresampled_mask_visual = resample_to_img(\n    mask_visual, mask, interpolation=\"nearest\"\n)\n\n# Plot the mask we will use\nplotting.plot_roi(\n    resampled_mask_visual,\n    title=\"Visual regions mask extracted from atlas\",\n    cut_coords=(8, -80, 9),\n    colorbar=False,\n    cmap=\"Paired\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define a masker\nWe define a nilearn masker that will be used to handle relevant data.\nFor more information, visit :\n'http://nilearn.github.io/manipulating_images/masker_objects.html'\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from nilearn.maskers import NiftiMasker\n\nroi_masker = NiftiMasker(mask_img=resampled_mask_visual).fit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare the data\nFor each subject, for each task and conditions, our dataset contains two\nindependent acquisitions, similar except for one acquisition parameter, the\nencoding phase used that was either Antero-Posterior (AP) or\nPostero-Anterior (PA). Although this induces small differences\nin the final data, we will take  advantage of these pseudo-duplicates to define training and test samples.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# The training set:\n# * source_train: AP acquisitions from source subjects (sub-01, sub-02).\n# * target_train: AP acquisitions from the target subject (sub-04).\n#\n\nsource_subjects = [sub for sub in sub_ids if sub != \"sub-04\"]\nsource_train = [\n    concat_imgs(df[(df.subject == sub) & (df.acquisition == \"ap\")].path.values)\n    for sub in source_subjects\n]\ntarget_train = concat_imgs(\n    df[(df.subject == \"sub-04\") & (df.acquisition == \"ap\")].path.values\n)\n\n# The testing set:\n# * source_test: PA acquisitions from source subjects (sub-01, sub-02).\n# * target test: PA acquisitions from the target subject (sub-04).\n#\n\nsource_test = [\n    concat_imgs(df[(df.subject == sub) & (df.acquisition == \"pa\")].path.values)\n    for sub in source_subjects\n]\ntarget_test = concat_imgs(\n    df[(df.subject == \"sub-04\") & (df.acquisition == \"pa\")].path.values\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choose the number of regions for local alignment\nFirst, as we will proceed to local alignment we choose a suitable number of\nregions so that each of them is approximately 100 voxels wide. Then our\nestimator will first make a functional clustering of voxels based on train\ndata to divide them into meaningful regions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nn_voxels = roi_masker.mask_img_.get_fdata().sum()\nprint(f\"The chosen region of interest contains {n_voxels} voxels\")\nn_pieces = int(np.round(n_voxels / 100))\nprint(f\"We will cluster them in {n_pieces} regions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Extract the parcels labels\nWe will use intra-parcel alignments, fmralign provides a simple utility\nto extract the labels of associated to each voxel:\n:func:`!fmralign.embeddings.parcellation.get_labels`.\nWe then organize the data in dictionaries of pairs of subjects names and data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign.embeddings.parcellation import get_labels\n\nlabels = get_labels(source_train, roi_masker, n_pieces)\ndict_source_train = dict(\n    zip(source_subjects, [roi_masker.transform(img) for img in source_train])\n)\ndict_source_test = dict(\n    zip(source_subjects, [roi_masker.transform(img) for img in source_test])\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Define the estimators, fit them and do a prediction\nOn each region, we search for a transformation R that is either :\n\n  *  orthogonal, i.e. R orthogonal, scaling sc s.t. ||sc RX - Y ||^2 is minimized\n  *  the optimal transport plan, which yields the minimal transport cost\n     while respecting the mass conservation constraints. Calculated with\n     entropic regularization.\n  *  the shared response model (SRM), which computes a shared response space\n     from different subjects, and then projects individual subject data into it.\n\nThen for each method we define the estimator, fit it, predict the new image and plot\nits correlation with the real signal. We use the identity (euclidean averaging)\nas the baseline.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from fmralign import GroupAlignment\nfrom fmralign.metrics import score_voxelwise\n\nmethods = [\"identity\", \"procrustes\", \"ot\", \"SRM\"]\n\n# Prepare to store the results\ntitles, aligned_scores = [], []\n\nfor i, method in enumerate(methods):\n    # Fit the group estimator on the training data\n    group_estimator = GroupAlignment(method=method, labels=labels).fit(\n        dict_source_train\n    )\n    # Compute a mapping between the template and the new subject\n    # using `target_train` and make a prediction using the left-out-data\n    target_pred = group_estimator.predict_subject(\n        dict_source_test, roi_masker.transform(target_train)\n    )\n\n    # Derive correlation between prediction, test\n    method_error = score_voxelwise(\n        target_test,\n        roi_masker.inverse_transform(target_pred),\n        masker=roi_masker,\n        loss=\"corr\",\n    )\n\n    # Store the results for plotting later\n    aligned_score = roi_masker.inverse_transform(method_error)\n    titles.append(f\"Correlation of prediction after {method} alignment\")\n    aligned_scores.append(aligned_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(4, 1, figsize=(8, 12))\n\nfor i, (score, title) in enumerate(zip(aligned_scores, titles)):\n    plotting.plot_stat_map(\n        score,\n        display_mode=\"z\",\n        cut_coords=[-15, -5],\n        vmax=1,\n        title=title,\n        axes=axes[i],\n        colorbar=True,\n    )\n\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary:\nWe compared TemplateAlignment methods (scaled orthogonal, optimal transport)\nwith SRM-based alignment on visual cortex activity.\nYou can see that SRM introduces more smoothness in the transformation,\nresulting in slightly higher correlation values.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}